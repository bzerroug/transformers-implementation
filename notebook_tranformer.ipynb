{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute self attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In transformers models, attention provides context for each sequence. This helps the model understand how different words relate to each other to create meaningful sentences. According to Wikipedia‚Äôs description, ‚Äúthe attention layer can access all previous states and weigh them according to a learned measure of relevance, providing relevant information about far-away tokens.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to ‚ÄúAttention Is All You Need‚Äù:\n",
    "\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "We call our particular attention ‚ÄúScaled Dot-Product Attention‚Äù. The input consists of queries and keys of dimension d_key, and values of dimension d_value. We compute the dot products of the query with all keys, divide each by ‚àö(d_key), and apply a softmax function to obtain the weights on the values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an input sequence with t k-dimensional vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"self_attention.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the attention:\n",
    "1. we need to compute the dot product of the matrix in order to obtain the weights.\n",
    "2. Then we apply softmax function to these weights to have the normalized weights\n",
    "3. Then we multiply these weights by initial vectors and sum them all to have the self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 10\n",
    "k = 5\n",
    "X = torch.rand(t, k)\n",
    "\n",
    "raw_weights = torch.mm(X, X.transpose(1, 0))\n",
    "weights = nn.Softmax(dim=1)(raw_weights)\n",
    "\n",
    "attention = torch.mm(weights, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5358, 0.4512, 0.4647, 0.6081, 0.5569],\n",
       "        [0.5740, 0.4100, 0.4658, 0.5725, 0.6087],\n",
       "        [0.5454, 0.3678, 0.4956, 0.5380, 0.6225],\n",
       "        [0.6130, 0.3393, 0.4469, 0.4723, 0.6416],\n",
       "        [0.6616, 0.3441, 0.4753, 0.4769, 0.6055],\n",
       "        [0.6666, 0.3788, 0.4809, 0.4748, 0.5598],\n",
       "        [0.6471, 0.3382, 0.4975, 0.4762, 0.5990],\n",
       "        [0.5740, 0.3639, 0.5166, 0.5478, 0.5777],\n",
       "        [0.6477, 0.4312, 0.4594, 0.5385, 0.6006],\n",
       "        [0.6579, 0.3818, 0.5083, 0.4755, 0.5660]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every input vector ùê±i is used in three different ways in the self attention operation:  \n",
    "\n",
    "- It is compared to every other vector to establish the weights for its own output ùê≤i  \n",
    "- It is compared to every other vector to establish the weights for the output of the j-th vector ùê≤j  \n",
    "- It is used as part of the weighted sum to compute each output vector once the weights have been established.  \n",
    "\n",
    "These roles are often called the query, the key and the value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make its life a little easier by deriving new vectors for each role, by applying a linear transformation to the original input vector. In other words, we add three k√ók weight matrices ùêñq, ùêñk,ùêñv and compute three linear transformations of each xi, for the three different parts of the self attention:\n",
    "- ùê™i=ùêñqùê±i \n",
    "- ùê§i=ùêñkùê±i\n",
    "- ùêØi=ùêñvùê±i\n",
    "\n",
    "w‚Ä≤ij=ùê™iTùê§j    \n",
    "\n",
    "wij=softmax(w‚Ä≤ij)   \n",
    "\n",
    "ùê≤i=‚àëjwijùêØj  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"key-query-value.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutli head attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to understand multi-head self-attention is to see it as a small number of copies of the self-attention mechanism applied in parallel, each with their own key, value and query transformation.  \n",
    "Each head receives low-dimensional keys queries and values. If the input vector has k=256 dimensions, and we have h=4 attention heads, we multiply the input vectors by a 256√ó64 matrix to project them down to a sequence of 64 dimansional vectors. For every head, we do this 3 times: for the keys, the queries and the values.  \n",
    "We project the initial vectors in lower dimension. The objective is to have multiple representations of the same vector that we will concatenate in the end to have all these representations in one place."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"multihead_attention.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires 3h matrices of size k by k/h. In total, this gives us 3hk(k/h)=3k^2 parameters to compute the inputs to the multi-head self-attention: the same as we had for the single-head self-attention.\n",
    "\n",
    "We can even implement this with just three k√ók matrix multiplications as in the single-head self-attention. The only extra operation we need is to slice the resulting sequence of vectors into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"compute_query_sequentially.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"compute_query_in_once.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How to build a multihead selfAttention Module:\n",
    "1. Define a head number that is divisible from the input \n",
    "2. Define linear transformations to key, queries and values for each head\n",
    "3. Apply the linear transformation associated to every input to obtain the key, query and value\n",
    "4. Reshape the matrix of key, query and value to have them in different heads. One dimension for heads: we can access to the input of each head\n",
    "5. Merge heads and batch because it's the same operation for each head \n",
    "6. Compute the raw weights w‚Ä≤ij=ùê™iTùê§j and normalize them (because the softmax function can be sensitive to very large input values. These kill the gradient, and slow down learning, or cause it to stop altogether)\n",
    "7. We apply the Softmax function\n",
    "8. Multiply weights of self attention to the values\n",
    "9. Reshape in order to concatenatre heads and have b x t x k\n",
    "10. Apply the unifyheads an return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultHeadsSelfAttentionOpt(nn.Module):\n",
    "    # 1.Define a head number that is divisible from the input \n",
    "    def __init__(self, k, heads=4, mask=False):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k = k\n",
    "        self.heads = heads\n",
    "            \n",
    "        # 2. Define linear transformations to key, queries and values for each head\n",
    "        # biais = False because we want only weights\n",
    "        self.to_queries = nn.Linear(k, k, bias=False)\n",
    "        self.to_keys    = nn.Linear(k, k, bias=False) \n",
    "        self.to_values  = nn.Linear(k, k, bias=False)\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size() #as the training will be done by batch\n",
    "\n",
    "        # 3. Apply the linear transformation associated to every input to obtain the key, query and value\n",
    "        query = self.to_queries(x)\n",
    "        key = self.to_keys(x)\n",
    "        value = self.to_values(x)\n",
    "        \n",
    "        s = self.k // self.heads # number of elements per head\n",
    "        h = self.heads\n",
    "\n",
    "        # 4. Reshape the matrix of key, query and value to have them in different heads. \n",
    "        queries = query.view(b, t, h, s)\n",
    "        keys = key.view(b, t, h, s)\n",
    "        values = value.view(b, t, h, s)\n",
    "\n",
    "        # 5. Merge heads and batch because it's the same operation for each head\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        # 6. Compute the raw weights w‚Ä≤ij=ùê™iTùê§j and normalize them\n",
    "        weights_raw = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        weights_raw_normalized = torch.div(weights_raw, torch.sqrt(torch.tensor(k)))\n",
    "\n",
    "        # 7. We apply the Softmax function to the similarity dimension (batch dim x input dim x sim dim)\n",
    "        weights = nn.Softmax(dim=2)(weights_raw_normalized)\n",
    "\n",
    "        # 8. Multiply weights of self attention to the values\n",
    "        self_attentions = torch.bmm(weights, values).view(b, h, t, s)\n",
    "\n",
    "        # 9. Reshape in order to concatenatre heads and have b x t x k\n",
    "        self_attention_formatted = self_attentions.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "\n",
    "        # 10. Apply the unifyheads an return it\n",
    "        return self.unifyheads(self_attention_formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, k, low_dim):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        self.k = k    \n",
    "        self.low_dim = low_dim \n",
    "        # 1. Define linear transformations to reduce dimensionnalit√© of input\n",
    "        # biais = False because we want only weights\n",
    "        self.to_reduce_dim = nn.Linear(k, low_dim, bias=False)\n",
    "        # 2. Define linear transformations to key, queries and values\n",
    "        # biais = False because we want only weights\n",
    "        self.to_queries = nn.Linear(low_dim, low_dim, bias=False)\n",
    "        self.to_keys    = nn.Linear(low_dim, low_dim, bias=False) \n",
    "        self.to_values  = nn.Linear(low_dim, low_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size() #as the training will be done by batch\n",
    "\n",
    "        # 3. Reduce dimensionnalit√© of input\n",
    "        low_dim_x = self.to_reduce_dim(x)\n",
    "        \n",
    "        # 4. Apply the linear transformation associated to every input to obtain the key, query and value\n",
    "        query = self.to_queries(low_dim_x) # b, t, low_dim\n",
    "        key = self.to_keys(low_dim_x)\n",
    "        value = self.to_values(low_dim_x)\n",
    "\n",
    "        # 5. Compute the raw weights w‚Ä≤ij=ùê™iTùê§j and normalize them\n",
    "        weights_raw = torch.bmm(query, key.transpose(1, 2))\n",
    "        weights_raw_normalized = torch.div(weights_raw, torch.sqrt(torch.tensor(self.low_dim)))\n",
    "\n",
    "        # 6. We apply the Softmax function to the similarity dimension (batch dim x input dim x sim dim)\n",
    "        weights = nn.Softmax(dim=2)(weights_raw_normalized)\n",
    "\n",
    "        # 7. Multiply weights of self attention to the values\n",
    "        return torch.bmm(weights, value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    # 8.Define a head number that is divisible from the input \n",
    "    def __init__(self, k, heads=4):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k = k\n",
    "        self.heads = heads  \n",
    "\n",
    "        # 9. Instantiate OneHeadSelfAttention multiple times to have MultiHeadSelfAttention\n",
    "        self.list_heads = []\n",
    "        for head in range(self.heads):\n",
    "            self.list_heads.append(OneHeadSelfAttention(k, k//heads))\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 10. Get all heads elements \n",
    "        list_to_concat = []\n",
    "        for one_head in self.list_heads:\n",
    "            list_to_concat.append((one_head(x),))\n",
    "\n",
    "        # 11. Concatenate all the heads\n",
    "        multi_heads = sum(list_to_concat, ())        \n",
    "        concatenated = torch.cat(multi_heads, dim=2)\n",
    "\n",
    "        # 12. Linear transformation\n",
    "        return self.unifyheads(concatenated)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHeadSelfAttentionQKV(nn.Module):\n",
    "    def __init__(self, k, low_dim):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        self.k = k    \n",
    "        self.low_dim = low_dim \n",
    "        # 1. Define linear transformations to reduce dimensionnalit√© of input\n",
    "        # biais = False because we want only weights\n",
    "        self.to_reduce_dim = nn.Linear(k, low_dim, bias=False)\n",
    "        # 2. Define linear transformations to key, queries and values\n",
    "        # biais = False because we want only weights\n",
    "        self.to_queries = nn.Linear(low_dim, low_dim, bias=False)\n",
    "        self.to_keys    = nn.Linear(low_dim, low_dim, bias=False) \n",
    "        self.to_values  = nn.Linear(low_dim, low_dim, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # 3. Reduce dimensionnalit√© of input\n",
    "        low_dim_Q = self.to_reduce_dim(Q)\n",
    "        low_dim_K = self.to_reduce_dim(K)\n",
    "        low_dim_V = self.to_reduce_dim(V)\n",
    "\n",
    "        \n",
    "        # 4. Apply the linear transformation associated to every input to obtain the key, query and value\n",
    "        query = self.to_queries(low_dim_Q) \n",
    "        key = self.to_keys(low_dim_K)\n",
    "        value = self.to_values(low_dim_V)\n",
    "\n",
    "        # 5. Compute the raw weights w‚Ä≤ij=ùê™iTùê§j and normalize them\n",
    "        weights_raw = torch.bmm(query, key.transpose(1, 2))\n",
    "        weights_raw_normalized = torch.div(weights_raw, torch.sqrt(torch.tensor(self.low_dim)))\n",
    "\n",
    "        # 6. We apply the Softmax function to the similarity dimension (batch dim x input dim x sim dim)\n",
    "        weights = nn.Softmax(dim=2)(weights_raw_normalized)\n",
    "\n",
    "        # 7. Multiply weights of self attention to the values\n",
    "        return torch.bmm(weights, value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionQKV(nn.Module):\n",
    "    # 8.Define a head number that is divisible from the input \n",
    "    def __init__(self, k, heads=4):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k = k\n",
    "        self.heads = heads  \n",
    "\n",
    "        # 9. Instantiate OneHeadSelfAttention multiple times to have MultiHeadSelfAttention\n",
    "        self.list_heads = []\n",
    "        for head in range(self.heads):\n",
    "            self.list_heads.append(OneHeadSelfAttentionQKV(k, k//heads))\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        # 10. Get all heads elements \n",
    "        list_to_concat = []\n",
    "        for one_head in self.list_heads:\n",
    "            list_to_concat.append((one_head(Q, K, V),))\n",
    "\n",
    "        # 11. Concatenate all the heads\n",
    "        multi_heads = sum(list_to_concat, ())        \n",
    "        concatenated = torch.cat(multi_heads, dim=2)\n",
    "\n",
    "        # 12. Linear transformation\n",
    "        return self.unifyheads(concatenated)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultHeadsSelfAttentionOptQKV(nn.Module):\n",
    "    # 1.Define a head number that is divisible from the input \n",
    "    def __init__(self, k, heads=4, mask=False):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k = k\n",
    "        self.heads = heads\n",
    "            \n",
    "        # 2. Define linear transformations to key, queries and values for each head\n",
    "        # biais = False because we want only weights\n",
    "        self.to_queries = nn.Linear(k, k, bias=False)\n",
    "        self.to_keys    = nn.Linear(k, k, bias=False) \n",
    "        self.to_values  = nn.Linear(k, k, bias=False)\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        b, t, k = Q.size() #as the training will be done by batch\n",
    "\n",
    "        # 3. Apply the linear transformation associated to every input to obtain the key, query and value\n",
    "        query = self.to_queries(Q)\n",
    "        key = self.to_keys(K)\n",
    "        value = self.to_values(V)\n",
    "        \n",
    "        s = self.k // self.heads # number of elements per head\n",
    "        h = self.heads\n",
    "\n",
    "        # 4. Reshape the matrix of key, query and value to have them in different heads. \n",
    "        queries = query.view(b, t, h, s)\n",
    "        keys = key.view(b, t, h, s)\n",
    "        values = value.view(b, t, h, s)\n",
    "\n",
    "        # 5. Merge heads and batch because it's the same operation for each head\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        # 6. Compute the raw weights w‚Ä≤ij=ùê™iTùê§j and normalize them\n",
    "        weights_raw = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        weights_raw_normalized = torch.div(weights_raw, torch.sqrt(torch.tensor(k)))\n",
    "\n",
    "        # 7. We apply the Softmax function to the similarity dimension (batch dim x input dim x sim dim)\n",
    "        weights = nn.Softmax(dim=2)(weights_raw_normalized)\n",
    "\n",
    "        # 8. Multiply weights of self attention to the values\n",
    "        self_attentions = torch.bmm(weights, values).view(b, h, t, s)\n",
    "\n",
    "        # 9. Reshape in order to concatenatre heads and have b x t x k\n",
    "        self_attention_formatted = self_attentions.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "\n",
    "        # 10. Apply the unifyheads an return it\n",
    "        return self.unifyheads(self_attention_formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(32, 1000, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 64])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = OneHeadSelfAttentionQKV(256, 64)(X, X, X)\n",
    "A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 s, sys: 285 ms, total: 2.23 s\n",
      "Wall time: 1.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 256])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "B = MultiHeadSelfAttentionQKV(k=256, heads=4)(X, X, X)\n",
    "B.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 586 ms, total: 2.45 s\n",
      "Wall time: 1.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 256])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "MultHeadsSelfAttentionOptQKV(256, 4)(X, X, X).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-implementation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

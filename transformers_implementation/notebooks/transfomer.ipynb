{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bachir/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenize import tokenize\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHeadSelfAttentionQKV(nn.Module):\n",
    "    def __init__(self, k, low_dim):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        self.k = k    \n",
    "        self.low_dim = low_dim \n",
    "        # 1. Define linear transformations to reduce dimensionnalit√© of input\n",
    "        # biais = False because we want only weights\n",
    "        self.to_reduce_dim = nn.Linear(k, low_dim, bias=False)\n",
    "        # 2. Define linear transformations to key, queries and values\n",
    "        # biais = False because we want only weights\n",
    "        self.to_queries = nn.Linear(low_dim, low_dim, bias=False)\n",
    "        self.to_keys    = nn.Linear(low_dim, low_dim, bias=False) \n",
    "        self.to_values  = nn.Linear(low_dim, low_dim, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        # 3. Reduce dimensionnalit√© of input\n",
    "        low_dim_Q = self.to_reduce_dim(Q)\n",
    "        low_dim_K = self.to_reduce_dim(K)\n",
    "        low_dim_V = self.to_reduce_dim(V)\n",
    "\n",
    "        \n",
    "        # 4. Apply the linear transformation associated to every input to obtain the key, query and value\n",
    "        query = self.to_queries(low_dim_Q) \n",
    "        key = self.to_keys(low_dim_K)\n",
    "        value = self.to_values(low_dim_V)\n",
    "\n",
    "        # 5. Compute the raw weights w‚Ä≤ij=ùê™iTùê§j and normalize them\n",
    "        weights_raw = torch.bmm(query, key.transpose(1, 2))\n",
    "        \n",
    "        # 5.a apply mask\n",
    "        if mask is not None:\n",
    "            weights_raw = weights_raw.masked_fill_(mask.logical_not(), float(\"-1e20\"))\n",
    "\n",
    "        weights_raw_normalized = torch.div(weights_raw, torch.sqrt(torch.tensor(self.low_dim)))\n",
    "\n",
    "        # 6. We apply the Softmax function to the similarity dimension (batch dim x input dim x sim dim)\n",
    "        weights = nn.Softmax(dim=2)(weights_raw_normalized)\n",
    "\n",
    "        # 7. Multiply weights of self attention to the values\n",
    "        return torch.bmm(weights, value)\n",
    "    \n",
    "\n",
    "class MultiHeadSelfAttentionQKV(nn.Module):\n",
    "    # 8.Define a head number that is divisible from the input \n",
    "    def __init__(self, k, heads=4):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k = k\n",
    "        self.heads = heads  \n",
    "\n",
    "        # 9. Instantiate OneHeadSelfAttention multiple times to have MultiHeadSelfAttention\n",
    "        self.list_heads = []\n",
    "        for head in range(self.heads):\n",
    "            self.list_heads.append(OneHeadSelfAttentionQKV(k, k//heads))\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # 10. Get all heads elements \n",
    "        list_to_concat = []\n",
    "        for one_head in self.list_heads:\n",
    "            list_to_concat.append((one_head(Q, K, V, mask),))\n",
    "\n",
    "        # 11. Concatenate all the heads\n",
    "        multi_heads = sum(list_to_concat, ())        \n",
    "        concatenated = torch.cat(multi_heads, dim=2)\n",
    "\n",
    "        # 12. Linear transformation\n",
    "        return self.unifyheads(concatenated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 100, 256])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(12, 100, 256)\n",
    "mask  = torch.tril(torch.ones((100, 100)), diagonal=1).bool()\n",
    "M = MultiHeadSelfAttentionQKV(256, 4)\n",
    "M(X, X, X, mask=mask).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dimension):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, dimension)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.word_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dimension, max_seq_length=2000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        positional_encoding = torch.zeros(max_seq_length, dimension)    \n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(dimension):\n",
    "                if i%2 == 0:\n",
    "                    pe = math.sin(pos / 1000**(2*i/dimension))\n",
    "                else:\n",
    "                    pe = math.cos(pos / 1000**(2*i/dimension))\n",
    "                positional_encoding[pos, i] = pe\n",
    "\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:x.size(1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, factor=2):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, factor*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(factor*embed_dim, embed_dim)\n",
    "        )  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/encoder.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncodingBloc(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, factor):\n",
    "        super(TransformerEncodingBloc, self).__init__()\n",
    "        # Mutli Head Attention with its notmalization and its dropout\n",
    "        self.attention = MultiHeadSelfAttentionQKV(embedding_dim, num_heads)\n",
    "        self.normalization_mha = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_mha = nn.Dropout(0.2)\n",
    "\n",
    "        # Feed Forward with its notmalization and its dropout\n",
    "        self.feed_forward = FeedForward(embedding_dim, factor)\n",
    "        self.normalization_ff = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_ff = nn.Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Multi Head Attention\n",
    "        mha = self.attention(query, key, value)\n",
    "        mha_residuals = mha + value\n",
    "        mha_residuals_norm = self.normalization_mha(mha_residuals)\n",
    "        mha_residuals_norm_dropout = self.dropout_mha(mha_residuals_norm)\n",
    "\n",
    "        # Feed Forward\n",
    "        ff = self.feed_forward(mha_residuals_norm_dropout)\n",
    "        ff_residuals = ff + mha_residuals_norm_dropout\n",
    "        ff_residuals_norm = self.normalization_ff(ff_residuals)\n",
    "        ff_residuals_norm_dropout = self.dropout_ff(ff_residuals_norm)\n",
    "\n",
    "        return ff_residuals_norm_dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, factor):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(embedding_dim)\n",
    "        self.transformer_layers = nn.ModuleList([TransformerEncodingBloc(embedding_dim, num_heads, factor) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.embedding(X)\n",
    "        output = self.positional_embedding(encoded)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            output = transformer_layer(output, output, output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randint(low=0, high=100, size=(10, 100))\n",
    "\n",
    "encoder = Encoder(vocab_size=5000, \n",
    "                  embedding_dim=256, \n",
    "                  num_heads=4,\n",
    "                  #dropout=0.2,\n",
    "                  num_layers=5,\n",
    "                  factor=2\n",
    "                  )\n",
    "\n",
    "Y = encoder(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 21])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "    \"In a cold and gray Chicago morning, a poor little baby child is born in the ghetto\",\n",
    "    \"I'll be there for you and the rain starts to pour\",\n",
    "    \"I like big butts and I cannot lie\",\n",
    "]\n",
    "\n",
    "encoded_input = tokenizer(batch_sentences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "encoded_input.input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.3442e-02, -1.0470e-01, -1.9869e-01,  ...,  4.2391e-01,\n",
       "          -2.6027e-01, -6.0122e-01],\n",
       "         [ 5.6368e-04, -3.5754e-01,  1.3343e+00,  ..., -4.8053e-01,\n",
       "           4.5416e-01, -6.9387e-02],\n",
       "         [ 2.6932e-01,  2.6601e-01, -0.0000e+00,  ...,  1.2859e+00,\n",
       "           0.0000e+00, -0.0000e+00],\n",
       "         ...,\n",
       "         [-1.6117e-01, -5.7511e-01, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           2.6810e-01, -3.5211e-01],\n",
       "         [-2.9852e-02,  4.5394e-03, -1.4956e-01,  ...,  4.5028e-02,\n",
       "          -1.5938e-01, -9.3726e-01],\n",
       "         [ 0.0000e+00, -3.4367e-01, -1.9233e-01,  ...,  7.3635e-02,\n",
       "          -4.2149e-01, -2.2788e-01]],\n",
       "\n",
       "        [[ 1.9344e-02, -4.1531e-01, -1.2358e-02,  ...,  0.0000e+00,\n",
       "           4.0751e+00, -1.0246e-01],\n",
       "         [-0.0000e+00, -1.7516e-01, -0.0000e+00,  ...,  3.1647e-02,\n",
       "          -4.1913e+00, -0.0000e+00],\n",
       "         [ 4.6181e-02, -5.6651e+00, -2.8903e-01,  ...,  3.3248e-01,\n",
       "          -2.3646e-01,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 1.3179e+00, -5.3928e-01, -2.7649e-01,  ..., -5.4720e-01,\n",
       "          -2.2671e-01,  0.0000e+00],\n",
       "         [-2.2022e-01,  3.8996e-01, -1.2455e-01,  ..., -5.7559e-01,\n",
       "          -1.6681e-01, -4.0363e-02],\n",
       "         [-0.0000e+00,  5.0400e-01, -7.0838e-01,  ...,  4.9382e-01,\n",
       "          -2.8337e-01,  4.9212e-01]],\n",
       "\n",
       "        [[ 0.0000e+00, -0.0000e+00, -1.0661e+00,  ..., -0.0000e+00,\n",
       "          -1.5217e-01,  0.0000e+00],\n",
       "         [-2.5892e+00, -0.0000e+00, -5.0464e-01,  ..., -1.3594e+00,\n",
       "          -2.4288e-01,  9.4965e-01],\n",
       "         [-0.0000e+00, -4.8677e-01, -5.3326e-01,  ...,  6.1067e-01,\n",
       "           9.8888e-01, -2.8301e+00],\n",
       "         ...,\n",
       "         [-1.8906e-01, -1.6568e+00, -3.0745e-01,  ...,  1.1113e+00,\n",
       "           9.8636e-01,  2.2379e-01],\n",
       "         [ 2.3165e-02,  4.2294e-02, -6.2610e-01,  ...,  1.2113e+00,\n",
       "           5.8686e-01, -0.0000e+00],\n",
       "         [ 5.5370e-02, -7.1009e-01, -5.4112e-01,  ...,  1.3245e-01,\n",
       "          -4.3303e-01,  5.7641e-01]],\n",
       "\n",
       "        [[-2.2388e-01, -1.2256e+00, -5.5740e-01,  ...,  7.5591e-02,\n",
       "           6.4077e-01,  9.6790e-01],\n",
       "         [-1.0806e-01, -6.4998e+00, -0.0000e+00,  ..., -2.8136e-01,\n",
       "          -2.2521e-01, -0.0000e+00],\n",
       "         [ 0.0000e+00,  9.4685e-03, -6.9909e-01,  ...,  5.6810e-02,\n",
       "           8.5252e-02,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 2.3575e-01, -1.5697e-01, -4.6999e+00,  ..., -4.0848e-01,\n",
       "          -3.5175e-02, -2.6673e+00],\n",
       "         [-1.2218e-01, -3.1228e+00, -4.8228e-01,  ...,  0.0000e+00,\n",
       "           5.7327e-01, -1.4441e-01],\n",
       "         [-1.0740e-01,  8.2827e-02, -1.4440e-01,  ...,  2.3314e-03,\n",
       "          -0.0000e+00, -5.4533e-01]],\n",
       "\n",
       "        [[ 1.5809e-01, -1.2596e+00, -2.0878e+00,  ...,  9.6662e-02,\n",
       "           8.2043e-01,  1.0928e+00],\n",
       "         [ 8.5455e-02, -0.0000e+00,  0.0000e+00,  ..., -1.9741e-01,\n",
       "          -8.6209e-02,  2.7275e-01],\n",
       "         [-5.2617e-02, -3.0159e-01,  3.9367e-01,  ...,  0.0000e+00,\n",
       "          -0.0000e+00,  1.3524e+00],\n",
       "         ...,\n",
       "         [-1.0795e-01,  0.0000e+00, -0.0000e+00,  ...,  5.4253e-01,\n",
       "          -0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00, -5.6435e-01,  1.3157e+00,  ..., -0.0000e+00,\n",
       "          -0.0000e+00, -9.1515e-03],\n",
       "         [ 7.1858e-02,  6.3942e-01,  2.3777e-01,  ..., -2.5556e-01,\n",
       "          -6.1103e+00, -3.2477e-01]],\n",
       "\n",
       "        [[-1.5609e-01, -3.7790e-02, -0.0000e+00,  ...,  0.0000e+00,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         [-0.0000e+00,  0.0000e+00,  4.3788e+00,  ...,  2.7645e-01,\n",
       "          -4.0195e-01,  4.4040e-01],\n",
       "         [ 6.0266e-01, -0.0000e+00, -2.4562e-01,  ...,  5.8526e-01,\n",
       "           1.2659e-01,  7.8791e-02],\n",
       "         ...,\n",
       "         [ 6.9345e-01, -1.7490e+00,  8.3016e-01,  ...,  4.9078e-01,\n",
       "          -1.9547e-01,  6.3628e-02],\n",
       "         [ 0.0000e+00, -2.1099e-01, -4.1925e-01,  ...,  8.6428e-01,\n",
       "          -3.0204e-01, -4.4781e-01],\n",
       "         [-7.4736e-01, -3.0159e-01, -3.3357e-01,  ..., -2.2026e-02,\n",
       "          -3.6590e-01,  1.0719e-01]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=tokenizer.vocab_size, \n",
    "                  embedding_dim=256, \n",
    "                  num_heads=4,\n",
    "                  num_layers=5,\n",
    "                  factor=2\n",
    "                  )\n",
    "\n",
    "encoder(encoded_input.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/decoder.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecodingBloc(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, factor):\n",
    "        super(TransformerDecodingBloc, self).__init__()\n",
    "        # Mutli Head Self Attention with its notmalization and its dropout\n",
    "        self.self_attention = MultiHeadSelfAttentionQKV(embedding_dim, num_heads)\n",
    "        self.normalization_mhsa = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_mhsa = nn.Dropout(0.2)\n",
    "\n",
    "        # Mutli Head cross Attention with its notmalization and its dropout\n",
    "        self.cross_attention = MultiHeadSelfAttentionQKV(embedding_dim, num_heads)\n",
    "        self.normalization_mhca = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_mhca = nn.Dropout(0.2)\n",
    "\n",
    "        # Feed Forward with its notmalization and its dropout\n",
    "        self.feed_forward = FeedForward(embedding_dim, factor)\n",
    "        self.normalization_ff = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_ff = nn.Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def forward(self, x, encoder, mask):\n",
    "        # Mutli Head Self Attention\n",
    "        mhsa = self.self_attention(x, x, x, mask)\n",
    "        mhsa_residuals = mhsa + x\n",
    "        mhsa_residuals_norm = self.normalization_mhsa(mhsa_residuals)\n",
    "        mhsa_residuals_norm_dropout = self.dropout_mhsa(mhsa_residuals_norm)\n",
    "\n",
    "        # Mutli Head Cross Attention\n",
    "        query = mhsa_residuals_norm_dropout\n",
    "        mhca = self.cross_attention(query, encoder, encoder) # Query belongs to self attention and Key and Value from encoder\n",
    "        mhca_residuals = mhca + query\n",
    "        mhca_residuals_norm = self.normalization_mhca(mhca_residuals)\n",
    "        mhca_residuals_norm_dropout = self.dropout_mhca(mhca_residuals_norm)\n",
    "\n",
    "         # Feed Forward with its notmalization and its dropout\n",
    "        ff = self.feed_forward(mhca_residuals_norm_dropout)\n",
    "        ff_residuals = ff + mhca_residuals_norm_dropout\n",
    "        ff_residuals_norm = self.normalization_ff(ff_residuals)\n",
    "        ff_residuals_norm_dropout = self.dropout_ff(ff_residuals_norm)\n",
    "\n",
    "        return ff_residuals_norm_dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, factor=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(embedding_dim)\n",
    "        self.transformer_layers = nn.ModuleList([TransformerDecodingBloc(embedding_dim, num_heads, factor) for i in range(num_layers)])\n",
    "        self.linear_out = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x, encoder_out, mask=None):\n",
    "        encoded = self.embedding(x)\n",
    "        output = self.positional_embedding(encoded)\n",
    "        output = self.dropout(output)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            output = transformer_layer(output, encoder_out, mask)\n",
    "            \n",
    "        output = self.linear_out(output)\n",
    "        \n",
    "        return nn.Softmax(dim=2)(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, num_heads, num_layers, factor=2):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_vocab_size, embedding_dim, num_heads, num_layers, factor)\n",
    "        self.decoder = Decoder(output_vocab_size, embedding_dim, num_heads, num_layers, factor)\n",
    "\n",
    "    def get_mask_output(self, output):\n",
    "        _, output_len = output.size()\n",
    "        return torch.tril(torch.ones((output_len, output_len)), diagonal=1).bool()\n",
    "    \n",
    "\n",
    "    def forward(self, input, output):\n",
    "        encoder = self.encoder(input) \n",
    "        mask = self.get_mask_output(output)\n",
    "        return self.decoder(output, encoder, mask=mask) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 21])\n",
      "torch.Size([6, 36])\n"
     ]
    }
   ],
   "source": [
    "batch_input = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "    \"In a cold and gray Chicago morning, a poor little baby child is born in the ghetto\",\n",
    "    \"I'll be there for you and the rain starts to pour\",\n",
    "    \"I like big butts and I cannot lie\",\n",
    "]\n",
    "\n",
    "batch_output = [\n",
    "    \"Pourquoi pas\",\n",
    "    \"Je ne pense pas\",\n",
    "    \"Je ne sais pas\",\n",
    "    \"J'ai pass√© la bague √† Chikita, deux mois apr√®s je l'ai d√©j√† quitt√©\",\n",
    "    \"H et Kaamelot sont les s√©ries fran√ßaises les plus marrantes\",\n",
    "    \"j'entends des bruits sur mon t√©l√©phone sans fil\",\n",
    "]\n",
    "\n",
    "tokenizer_input = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer_output = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer_input(batch_input, padding=True, return_tensors=\"pt\")\n",
    "print(encoded_input.input_ids.size())\n",
    "\n",
    "encoded_output = tokenizer_output(batch_output, padding=True, return_tensors=\"pt\")\n",
    "print(encoded_output.input_ids.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_output.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(input_vocab_size=tokenizer_input.vocab_size,\n",
    "                          output_vocab_size=tokenizer_output.vocab_size,\n",
    "                          embedding_dim=256, \n",
    "                          num_heads=4, \n",
    "                          num_layers=5, \n",
    "                          factor=2\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transformer(encoded_input.input_ids, encoded_output.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 36])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 36, 28996])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-implementation-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

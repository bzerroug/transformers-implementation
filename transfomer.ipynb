{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import tokenize\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHeadSelfAttentionQKV(nn.Module):\n",
    "    def __init__(self, k, low_dim):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        self.k = k    \n",
    "        self.low_dim = low_dim \n",
    "        # 1. Define linear transformations to reduce dimensionnalit√© of input\n",
    "        # biais = False because we want only weights\n",
    "        self.to_reduce_dim = nn.Linear(k, low_dim, bias=False)\n",
    "        # 2. Define linear transformations to key, queries and values\n",
    "        # biais = False because we want only weights\n",
    "        self.to_queries = nn.Linear(low_dim, low_dim, bias=False)\n",
    "        self.to_keys    = nn.Linear(low_dim, low_dim, bias=False) \n",
    "        self.to_values  = nn.Linear(low_dim, low_dim, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # 3. Reduce dimensionnalit√© of input\n",
    "        low_dim_Q = self.to_reduce_dim(Q)\n",
    "        low_dim_K = self.to_reduce_dim(K)\n",
    "        low_dim_V = self.to_reduce_dim(V)\n",
    "\n",
    "        \n",
    "        # 4. Apply the linear transformation associated to every input to obtain the key, query and value\n",
    "        query = self.to_queries(low_dim_Q) \n",
    "        key = self.to_keys(low_dim_K)\n",
    "        value = self.to_values(low_dim_V)\n",
    "\n",
    "        # 5. Compute the raw weights w‚Ä≤ij=ùê™iTùê§j and normalize them\n",
    "        weights_raw = torch.bmm(query, key.transpose(1, 2))\n",
    "        weights_raw_normalized = torch.div(weights_raw, torch.sqrt(torch.tensor(self.low_dim)))\n",
    "\n",
    "        # 6. We apply the Softmax function to the similarity dimension (batch dim x input dim x sim dim)\n",
    "        weights = nn.Softmax(dim=2)(weights_raw_normalized)\n",
    "\n",
    "        # 7. Multiply weights of self attention to the values\n",
    "        return torch.bmm(weights, value)\n",
    "    \n",
    "\n",
    "class MultiHeadSelfAttentionQKV(nn.Module):\n",
    "    # 8.Define a head number that is divisible from the input \n",
    "    def __init__(self, k, heads=4):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k = k\n",
    "        self.heads = heads  \n",
    "\n",
    "        # 9. Instantiate OneHeadSelfAttention multiple times to have MultiHeadSelfAttention\n",
    "        self.list_heads = []\n",
    "        for head in range(self.heads):\n",
    "            self.list_heads.append(OneHeadSelfAttentionQKV(k, k//heads))\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        # 10. Get all heads elements \n",
    "        list_to_concat = []\n",
    "        for one_head in self.list_heads:\n",
    "            list_to_concat.append((one_head(Q, K, V),))\n",
    "\n",
    "        # 11. Concatenate all the heads\n",
    "        multi_heads = sum(list_to_concat, ())        \n",
    "        concatenated = torch.cat(multi_heads, dim=2)\n",
    "\n",
    "        # 12. Linear transformation\n",
    "        return self.unifyheads(concatenated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dimension):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, dimension)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.word_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dimension, max_seq_length=2000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        positional_encoding = torch.zeros(max_seq_length, dimension)    \n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(dimension):\n",
    "                if i%2 == 0:\n",
    "                    pe = math.sin(pos / 1000**(2*i/dimension))\n",
    "                else:\n",
    "                    pe = math.cos(pos / 1000**(2*i/dimension))\n",
    "                positional_encoding[pos, i] = pe\n",
    "\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:x.size(1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, factor=2):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, factor*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(factor*embed_dim, embed_dim)\n",
    "        )  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"encoder.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncodingBloc(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout, factor=1):\n",
    "        super(TransformerEncodingBloc, self).__init__()\n",
    "        # Mutli Head Attention with its notmalization and its dropout\n",
    "        self.attention = MultiHeadSelfAttentionQKV(embedding_dim, num_heads)\n",
    "        self.normalization_mha = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_mha = nn.Dropout(dropout)\n",
    "\n",
    "        # Feed Forward with its notmalization and its dropout\n",
    "        self.feed_forward = FeedForward(embedding_dim, factor)\n",
    "        self.normalization_ff = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_ff = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        mha = self.attention(query, key, value)\n",
    "        mha_residuals = mha + value\n",
    "        mha_residuals_norm = self.normalization_mha(mha_residuals)\n",
    "        mha_residuals_norm_dropout = self.dropout_mha(mha_residuals_norm)\n",
    "\n",
    "        ff = self.feed_forward(mha_residuals_norm_dropout)\n",
    "        ff_residuals = ff + mha_residuals_norm_dropout\n",
    "        ff_residuals_norm = self.normalization_ff(ff_residuals)\n",
    "        ff_residuals_norm_dropout = self.dropout_ff(ff_residuals_norm)\n",
    "\n",
    "        return ff_residuals_norm_dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(10, 1000, 256)\n",
    "\n",
    "TEB = TransformerEncodingBloc(256, 4, 0.2, 1)\n",
    "Z = TEB(X, X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, dropout, num_layers, factor=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(embedding_dim)\n",
    "        self.transformer_layers = nn.ModuleList([TransformerEncodingBloc(embedding_dim, num_heads, dropout) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.embedding(X)\n",
    "        output = self.positional_embedding(encoded)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            output = transformer_layer(output, output, output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randint(low=0, high=100, size=(10, 100))\n",
    "\n",
    "encoder = Encoder(vocab_size=5000, \n",
    "                  embedding_dim=256, \n",
    "                  num_heads=4,\n",
    "                  dropout=0.2,\n",
    "                  num_layers=5\n",
    "                  )\n",
    "\n",
    "Y = encoder(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 256])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-implementation-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import tokenize\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHeadSelfAttentionQKV(nn.Module):\n",
    "    def __init__(self, k, low_dim):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        self.k = k    \n",
    "        self.low_dim = low_dim \n",
    "        # 1. Define linear transformations to reduce dimensionnalit√© of input\n",
    "        # biais = False because we want only weights\n",
    "        self.to_reduce_dim = nn.Linear(k, low_dim, bias=False)\n",
    "        # 2. Define linear transformations to key, queries and values\n",
    "        # biais = False because we want only weights\n",
    "        self.to_queries = nn.Linear(low_dim, low_dim, bias=False)\n",
    "        self.to_keys    = nn.Linear(low_dim, low_dim, bias=False) \n",
    "        self.to_values  = nn.Linear(low_dim, low_dim, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # 3. Reduce dimensionnalit√© of input\n",
    "        low_dim_Q = self.to_reduce_dim(Q)\n",
    "        low_dim_K = self.to_reduce_dim(K)\n",
    "        low_dim_V = self.to_reduce_dim(V)\n",
    "\n",
    "        \n",
    "        # 4. Apply the linear transformation associated to every input to obtain the key, query and value\n",
    "        query = self.to_queries(low_dim_Q) \n",
    "        key = self.to_keys(low_dim_K)\n",
    "        value = self.to_values(low_dim_V)\n",
    "\n",
    "        # 5. Compute the raw weights w‚Ä≤ij=ùê™iTùê§j and normalize them\n",
    "        weights_raw = torch.bmm(query, key.transpose(1, 2))\n",
    "        weights_raw_normalized = torch.div(weights_raw, torch.sqrt(torch.tensor(self.low_dim)))\n",
    "\n",
    "        # 6. We apply the Softmax function to the similarity dimension (batch dim x input dim x sim dim)\n",
    "        weights = nn.Softmax(dim=2)(weights_raw_normalized)\n",
    "\n",
    "        # 7. Multiply weights of self attention to the values\n",
    "        return torch.bmm(weights, value)\n",
    "    \n",
    "\n",
    "class MultiHeadSelfAttentionQKV(nn.Module):\n",
    "    # 8.Define a head number that is divisible from the input \n",
    "    def __init__(self, k, heads=4):\n",
    "        super().__init__()\n",
    "        # Check if input is divisible by number of heads\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k = k\n",
    "        self.heads = heads  \n",
    "\n",
    "        # 9. Instantiate OneHeadSelfAttention multiple times to have MultiHeadSelfAttention\n",
    "        self.list_heads = []\n",
    "        for head in range(self.heads):\n",
    "            self.list_heads.append(OneHeadSelfAttentionQKV(k, k//heads))\n",
    "\n",
    "        # This will be applied after the multi-head self-attention operation.\n",
    "        self.unifyheads = nn.Linear(k, k)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        # 10. Get all heads elements \n",
    "        list_to_concat = []\n",
    "        for one_head in self.list_heads:\n",
    "            list_to_concat.append((one_head(Q, K, V),))\n",
    "\n",
    "        # 11. Concatenate all the heads\n",
    "        multi_heads = sum(list_to_concat, ())        \n",
    "        concatenated = torch.cat(multi_heads, dim=2)\n",
    "\n",
    "        # 12. Linear transformation\n",
    "        return self.unifyheads(concatenated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dimension):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, dimension)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.word_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dimension, max_seq_length=2000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        positional_encoding = torch.zeros(max_seq_length, dimension)    \n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(dimension):\n",
    "                if i%2 == 0:\n",
    "                    pe = math.sin(pos / 1000**(2*i/dimension))\n",
    "                else:\n",
    "                    pe = math.cos(pos / 1000**(2*i/dimension))\n",
    "                positional_encoding[pos, i] = pe\n",
    "\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:x.size(1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, factor=2):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, factor*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(factor*embed_dim, embed_dim)\n",
    "        )  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"encoder.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncodingBloc(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout, factor=1):\n",
    "        super(TransformerEncodingBloc, self).__init__()\n",
    "        # Mutli Head Attention with its notmalization and its dropout\n",
    "        self.attention = MultiHeadSelfAttentionQKV(embedding_dim, num_heads)\n",
    "        self.normalization_mha = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_mha = nn.Dropout(dropout)\n",
    "\n",
    "        # Feed Forward with its notmalization and its dropout\n",
    "        self.feed_forward = FeedForward(embedding_dim, factor)\n",
    "        self.normalization_ff = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_ff = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        mha = self.attention(query, key, value)\n",
    "        mha_residuals = mha + value\n",
    "        mha_residuals_norm = self.normalization_mha(mha_residuals)\n",
    "        mha_residuals_norm_dropout = self.dropout_mha(mha_residuals_norm)\n",
    "\n",
    "        ff = self.feed_forward(mha_residuals_norm_dropout)\n",
    "        ff_residuals = ff + mha_residuals_norm_dropout\n",
    "        ff_residuals_norm = self.normalization_ff(ff_residuals)\n",
    "        ff_residuals_norm_dropout = self.dropout_ff(ff_residuals_norm)\n",
    "\n",
    "        return ff_residuals_norm_dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(10, 1000, 256)\n",
    "\n",
    "TEB = TransformerEncodingBloc(256, 4, 0.2, 1)\n",
    "Z = TEB(X, X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, dropout, num_layers, factor=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(embedding_dim)\n",
    "        self.transformer_layers = nn.ModuleList([TransformerEncodingBloc(embedding_dim, num_heads, dropout) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.embedding(X)\n",
    "        output = self.positional_embedding(encoded)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            output = transformer_layer(output, output, output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bachir/Documents/transformers-implementation/transfomer.ipynb Cellule 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(\u001b[39m10\u001b[39m, \u001b[39m1000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m encoder \u001b[39m=\u001b[39m Encoder(vocab_size\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                   embedding_dim\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                   num_heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                   dropout\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                   num_layers\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                   )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m Y \u001b[39m=\u001b[39m encoder(X)\n",
      "File \u001b[0;32m~/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/bachir/Documents/transformers-implementation/transfomer.ipynb Cellule 13\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding(encoded)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m transformer_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_layers:\n",
      "File \u001b[0;32m~/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/bachir/Documents/transformers-implementation/transfomer.ipynb Cellule 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bachir/Documents/transformers-implementation/transfomer.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embedding(x)\n",
      "File \u001b[0;32m~/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/Documents/transformers-implementation/transformers-implementation-env/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(10, 1000)\n",
    "\n",
    "encoder = Encoder(vocab_size=5000, \n",
    "                  embedding_dim=256, \n",
    "                  num_heads=4,\n",
    "                  dropout=0.2,\n",
    "                  num_layers=5\n",
    "                  )\n",
    "\n",
    "Y = encoder(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-implementation-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
